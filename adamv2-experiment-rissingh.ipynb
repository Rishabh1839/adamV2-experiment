{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":18613,"sourceType":"datasetVersion","datasetId":5839}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rishabhsingh18/adamv2-experiment-rissingh?scriptVersionId=191460691\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom sklearn.metrics import pairwise_distances, accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-08-06T19:22:22.44277Z","iopub.execute_input":"2024-08-06T19:22:22.443227Z","iopub.status.idle":"2024-08-06T19:22:22.455243Z","shell.execute_reply.started":"2024-08-06T19:22:22.443189Z","shell.execute_reply":"2024-08-06T19:22:22.453863Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Data Selection: for this experiment I have used the Chest X-Ray Datasets from the NIH which is also available on Kaggle. And use Torchvision for preprocessing the data.\n\nOn the other end I tried to organize the dataset by different subcategories as well in order to define which is which. \n","metadata":{}},{"cell_type":"code","source":"# Define paths\ndata_dir = '/kaggle/input/data'\nlabels_file = 'path_to_nih_chest_xray_labels_file.csv'\n\n# Step 1: Organize the dataset\ndef organize_nih_dataset(data_dir, labels_file):\n    images_dir = os.path.join(data_dir, 'images')\n    train_dir = os.path.join(data_dir, 'train')\n    test_dir = os.path.join(data_dir, 'test')\n    \n    if not os.path.exists(train_dir):\n        os.makedirs(train_dir)\n    \n    if not os.path.exists(test_dir):\n        os.makedirs(test_dir)\n    \n    labels = pd.read_csv(labels_file)\n    \n    # Create directories for each class\n    classes = labels['Finding Labels'].unique()\n    for cls in classes:\n        if cls == 'No Finding':\n            continue\n        os.makedirs(os.path.join(train_dir, cls), exist_ok=True)\n        os.makedirs(os.path.join(test_dir, cls), exist_ok=True)\n    \n    # Move images to corresponding directories\n    for idx, row in tqdm(labels.iterrows(), total=len(labels)):\n        file_name = row['Image Index']\n        label = row['Finding Labels']\n        \n        # Skip images without any findings\n        if label == 'No Finding':\n            continue\n        \n        # Split data into training and testing (e.g., 80-20 split)\n        if idx % 5 == 0:\n            dest_dir = test_dir\n        else:\n            dest_dir = train_dir\n        \n        # Move the image to the corresponding class directory\n        src_path = os.path.join(images_dir, file_name)\n        dest_path = os.path.join(dest_dir, label, file_name)\n        shutil.move(src_path, dest_path)\n        \n        # Step 2: Data Preparation\nbatch_size = 32\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'test'), transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T19:22:22.457969Z","iopub.execute_input":"2024-08-06T19:22:22.458851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we know we will be implementing the AdamV2 model with its 3 branches\n\n- Localizability - learns to distinguish different anatomical structures. Utilized student and teacher encoders to process images and generate embeddings, maximizing its consistency between the anchor image and its augmented views.\n\n- Composability - This branch learns part-whole relationships by assembling larger anatomical structures from smaller parts. Uses Student and teacher networks to process parts of an image, generating embeddings that represent the whole structure.\n\n- decomposability - learns whole-part relationships by decomposing larger structures into smaller parts. Processes the whole image and its parts to maximeze the agreement between the embeddings of the whole and its parts.\n\nhere is an example using a simple ResNet backbone.\n\nZero Shot Evaluation - is used to evaluate the model's ability to produce meaningful embeddings without any fine tuning. Use a nearest neighbor search to evaluate the embeddings.\n\n","metadata":{}},{"cell_type":"code","source":"# Model Architecture\nclass AdamV2(nn.Module):\n    def __init__(self):\n        super(AdamV2, self).__init__()\n        self.backbone = models.resnet50(weights=True)\n        self.backbone.fc = nn.Identity()  # Remove the last classification layer\n        \n        # Define heads for each branch\n        self.localizability_head = nn.Linear(2048, 128)\n        self.composability_head = nn.Linear(2048, 128)\n        self.decomposability_head = nn.Linear(2048, 128)\n    \n    def forward(self, x):\n        features = self.backbone(x)\n        localizability = self.localizability_head(features)\n        composability = self.composability_head(features)\n        decomposability = self.decomposability_head(features)\n        return localizability, composability, decomposability\n\nmodel = AdamV2().cuda()  # Move model to GPU if available","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Zero Shot evaluation","metadata":{}},{"cell_type":"code","source":"# Zero-shot Evaluation\nmodel.eval()\nembeddings = []\nlabels = []\n\n# Collect embeddings from the test dataset\nwith torch.no_grad():\n    for images, label in test_loader:\n        images = images.cuda()\n        localizability, composability, decomposability = model(images)\n        embeddings.append(localizability.cpu().numpy())\n        labels.append(label.numpy())\n\nembeddings = np.concatenate(embeddings, axis=0)\nlabels = np.concatenate(labels, axis=0)\n\n# Perform nearest neighbor search\ndef nearest_neighbor_accuracy(embeddings, labels, k=1):\n    distances = pairwise_distances(embeddings, embeddings, metric='euclidean')\n    sorted_indices = np.argsort(distances, axis=1)\n    nearest_labels = labels[sorted_indices[:, 1:k+1]]\n    \n    correct = 0\n    for i in range(len(labels)):\n        if labels[i] in nearest_labels[i]:\n            correct += 1\n            \n    return correct / len(labels)\n\naccuracy = nearest_neighbor_accuracy(embeddings, labels, k=1)\nprint(f'Zero-shot Nearest Neighbor Accuracy: {accuracy:.4f}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Few - shot Transfer learning - will fine-tune the model on a small labeled dataset and evaluate its performance.","metadata":{}},{"cell_type":"code","source":"# Few-shot Transfer Learning\nfew_shot_epochs = 5\nfew_shot_optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\n\nfor epoch in range(few_shot_epochs):\n    model.train()\n    for images, labels in train_loader:  # Replace `train_loader` with `few_shot_train_loader` if available\n        images, labels = images.cuda(), labels.cuda()\n        few_shot_optimizer.zero_grad()\n\n        # Forward pass\n        localizability, composability, decomposability = model(images)\n\n        # Use a classification loss for few-shot learning\n        classification_loss = nn.CrossEntropyLoss()(localizability, labels)\n\n        # Backward pass and optimization\n        classification_loss.backward()\n        few_shot_optimizer.step()\n\n    print(f'Few-shot Epoch [{epoch + 1}/{few_shot_epochs}], Loss: {classification_loss.item():.4f}')\n    # Evaluate on the test set\nmodel.eval()\nfew_shot_embeddings = []\nfew_shot_labels = []\n\nwith torch.no_grad():\n    for images, label in test_loader:\n        images = images.cuda()\n        localizability, composability, decomposability = model(images)\n        few_shot_embeddings.append(localizability.cpu().numpy())\n        few_shot_labels.append(label.numpy())\n\nfew_shot_embeddings = np.concatenate(few_shot_embeddings, axis=0)\nfew_shot_labels = np.concatenate(few_shot_labels, axis=0)\n\nfew_shot_accuracy = nearest_neighbor_accuracy(few_shot_embeddings, few_shot_labels, k=1)\nprint(f'Few-shot Nearest Neighbor Accuracy: {few_shot_accuracy:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Full fine-tuning the model on the entire labeled dataset","metadata":{}},{"cell_type":"code","source":"# Full Fine-tuning\nfull_tuning_epochs = 10\nfull_tuning_optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\nfor epoch in range(full_tuning_epochs):\n    model.train()\n    for images, labels in train_loader:\n        images, labels = images.cuda(), labels.cuda()\n        full_tuning_optimizer.zero_grad()\n\n        # Forward pass\n        localizability, composability, decomposability = model(images)\n\n        # Use a classification loss for full fine-tuning\n        classification_loss = nn.CrossEntropyLoss()(localizability, labels)\n\n        # Backward pass and optimization\n        classification_loss.backward()\n        full_tuning_optimizer.step()\n\n    print(f'Full Fine-tuning Epoch [{epoch + 1}/{full_tuning_epochs}], Loss: {classification_loss.item():.4f}')\n\n# Evaluate on the test set\nmodel.eval()\nfull_tuning_embeddings = []\nfull_tuning_labels = []\n\nwith torch.no_grad():\n    for images, label in test_loader:\n        images = images.cuda()\n        localizability, composability, decomposability = model(images)\n        full_tuning_embeddings.append(localizability.cpu().numpy())\n        full_tuning_labels.append(label.numpy())\n\nfull_tuning_embeddings = np.concatenate(full_tuning_embeddings, axis=0)\nfull_tuning_labels = np.concatenate(full_tuning_labels, axis=0)\n\nfull_tuning_accuracy = nearest_neighbor_accuracy(full_tuning_embeddings, full_tuning_labels, k=1)\nprint(f'Full Fine-tuning Nearest Neighbor Accuracy: {full_tuning_accuracy:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Analysis and Validation \n\nPerform Analyses to assess the model's understanding of anatomical structures. Use the t-SNE for visualizationg of embeddings.","metadata":{}},{"cell_type":"code","source":"# Feature Analysis\ndef plot_embeddings(embeddings, labels):\n    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n    tsne_results = tsne.fit_transform(embeddings)\n\n    plt.figure(figsize=(10, 10))\n    for label in np.unique(labels):\n        indices = np.where(labels == label)\n        plt.scatter(tsne_results[indices, 0], tsne_results[indices, 1], label=label)\n\n    plt.legend()\n    plt.show()\n\n\nplot_embeddings(full_tuning_embeddings, full_tuning_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Validation\n#def evaluate_classification_performance(labels_true, labels_pred):\n    #accuracy = accuracy_score(labels_true, labels_pred)\n    #f1 = f1_score(labels_true, labels_pred, average='weighted')\n    #precision = precision_score(labels_true, labels_pred, average='weighted')\n    #recall = recall_score(labels_true, labels_pred, average='weighted')\n\n    #print(f'Accuracy: {accuracy:.4f}')\n    #print(f'F1 Score: {f1:.4f}')\n    #print(f'Precision: {precision:.4f}')\n    #print(f'Recall: {recall:.4f}')\n\n#Assuming labels_pred are obtained from a classifier on top of embeddings\n#labels_pred = ...  # Obtain these from your classifier\n#evaluate_classification_performance(full_tuning_labels, labels_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References:\n\nSimCLR: A Simple Framework for Contrastive Learning of Visual Representations\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton\nPaper\nBYOL: Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning\n\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, Michal Valko\nPaper\nMoCo: Momentum Contrast for Unsupervised Visual Representation Learning\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick\nPaper\nHierarchical Representation Learning\nGLOM: How to represent part-whole hierarchies in a neural network\n\nGeoffrey Hinton\nPaper\nLearning Disentangled Representations with Semi-Supervised Deep Generative Models\n\nLars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, Ole Winther\nPaper\nMedical Image Analysis\nCheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning\n\nPranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, Matthew P. Lungren, Andrew Y. Ng\nPaper\nUNet: Convolutional Networks for Biomedical Image Segmentation\n\nOlaf Ronneberger, Philipp Fischer, Thomas Brox\nPaper\nGeneral Deep Learning\nDeep Learning\nIan Goodfellow, Yoshua Bengio, Aaron Courville\nBook\nFurther Reading and Tools\nPyTorch Documentation\n\nOfficial PyTorch documentation and tutorials\nWebsite\nKeras Documentation\n\nOfficial Keras documentation and tutorials\nWebsite\nSelf-Supervised Learning: The Dark Matter of Intelligence\n\nYann LeCun, Ishan Misra, Soumith Chintala, Sergey Zagoruyko\nPaper","metadata":{}}]}